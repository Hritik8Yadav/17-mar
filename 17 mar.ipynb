{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0fe3ac74-b516-4bc9-bcf1-b6b69e94a479",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''Ans1: The missing values are the values which doesn't have the complete details of a certain data. It is the \n",
    "            values which is not present due to certain circumstances such as the due to details not given by the\n",
    "            detailer etc. \n",
    "         \n",
    "         The missing values are need to be handled because the missing values can cause the improper dataset and \n",
    "         leads to the insufficient training data due to which a model can misbehave.\n",
    "         Some algorithms which aren't affected by the missing values are:\n",
    "             \n",
    "             i. decision tree\n",
    "             ii. k-nearest neighbour\n",
    "             iii. random forest\n",
    "             iv. naive baise.\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "1431ecf5-0ebb-4b23-86ca-b229c1e5bbee",
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "501d2b29-889c-4be3-9427-2a4877678521",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>survived</th>\n",
       "      <th>pclass</th>\n",
       "      <th>sex</th>\n",
       "      <th>age</th>\n",
       "      <th>sibsp</th>\n",
       "      <th>parch</th>\n",
       "      <th>fare</th>\n",
       "      <th>embarked</th>\n",
       "      <th>class</th>\n",
       "      <th>who</th>\n",
       "      <th>adult_male</th>\n",
       "      <th>deck</th>\n",
       "      <th>embark_town</th>\n",
       "      <th>alive</th>\n",
       "      <th>alone</th>\n",
       "      <th>fill_mean</th>\n",
       "      <th>fill_median</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>male</td>\n",
       "      <td>22.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>7.2500</td>\n",
       "      <td>S</td>\n",
       "      <td>Third</td>\n",
       "      <td>man</td>\n",
       "      <td>True</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Southampton</td>\n",
       "      <td>no</td>\n",
       "      <td>False</td>\n",
       "      <td>22.0</td>\n",
       "      <td>22.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>female</td>\n",
       "      <td>38.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>71.2833</td>\n",
       "      <td>C</td>\n",
       "      <td>First</td>\n",
       "      <td>woman</td>\n",
       "      <td>False</td>\n",
       "      <td>C</td>\n",
       "      <td>Cherbourg</td>\n",
       "      <td>yes</td>\n",
       "      <td>False</td>\n",
       "      <td>38.0</td>\n",
       "      <td>38.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>female</td>\n",
       "      <td>26.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>7.9250</td>\n",
       "      <td>S</td>\n",
       "      <td>Third</td>\n",
       "      <td>woman</td>\n",
       "      <td>False</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Southampton</td>\n",
       "      <td>yes</td>\n",
       "      <td>True</td>\n",
       "      <td>26.0</td>\n",
       "      <td>26.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>female</td>\n",
       "      <td>35.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>53.1000</td>\n",
       "      <td>S</td>\n",
       "      <td>First</td>\n",
       "      <td>woman</td>\n",
       "      <td>False</td>\n",
       "      <td>C</td>\n",
       "      <td>Southampton</td>\n",
       "      <td>yes</td>\n",
       "      <td>False</td>\n",
       "      <td>35.0</td>\n",
       "      <td>35.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>male</td>\n",
       "      <td>35.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>8.0500</td>\n",
       "      <td>S</td>\n",
       "      <td>Third</td>\n",
       "      <td>man</td>\n",
       "      <td>True</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Southampton</td>\n",
       "      <td>no</td>\n",
       "      <td>True</td>\n",
       "      <td>35.0</td>\n",
       "      <td>35.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   survived  pclass     sex   age  sibsp  parch     fare embarked  class  \\\n",
       "0         0       3    male  22.0      1      0   7.2500        S  Third   \n",
       "1         1       1  female  38.0      1      0  71.2833        C  First   \n",
       "2         1       3  female  26.0      0      0   7.9250        S  Third   \n",
       "3         1       1  female  35.0      1      0  53.1000        S  First   \n",
       "4         0       3    male  35.0      0      0   8.0500        S  Third   \n",
       "\n",
       "     who  adult_male deck  embark_town alive  alone fill_mean fill_median  \n",
       "0    man        True  NaN  Southampton    no  False      22.0        22.0  \n",
       "1  woman       False    C    Cherbourg   yes  False      38.0        38.0  \n",
       "2  woman       False  NaN  Southampton   yes   True      26.0        26.0  \n",
       "3  woman       False    C  Southampton   yes  False      35.0        35.0  \n",
       "4    man        True  NaN  Southampton    no   True      35.0        35.0  "
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''Ans2: The various techniques used for handling the missing values are:\n",
    "            \n",
    "            i. Deleting the missing data\n",
    "            ii. Mean value imputation\n",
    "            iii. Median value imputation\n",
    "            iv. Mode value impuatation\n",
    "'''\n",
    "dta=sb.load_dataset('titanic')\n",
    "dta['fill_mean']=dta['age'].fillna(dta['age'].mean)\n",
    "dta['fill_median']=dta['age'].fillna(dta['age'].median)\n",
    "dta.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0955464c-a97f-49e9-8b2c-c14dabdb92ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''Ans3: The imbalanced dataset is the dataset which doesn't have the equal number of interations in the two or more\n",
    "            features. for example the dataset of height of 10th and 12th class and if the 12 class has 100 number of \n",
    "            data while the 10th have the 30 then the dataset is said to be balanced.\n",
    "            \n",
    "         The imbalanced dataset can cause the model to be biased towards the feature having the more data this can \n",
    "         cause the model to give the output only for the feature having the more data.\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2a05cc3-98fa-4a6c-a5f3-efee027a2f12",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''Ans4: The Up-sampling and Down-sampling are the two techniques used to handle the imbalanced datasets. The two\n",
    "            techniques are totally opposite to each other.\n",
    "            \n",
    "         Up-sampling:- This technique involves the increasing the number of data of the datasets which has lesser \n",
    "                       number of datapoints by creating the the replication of the datapoints. for example the \n",
    "                       increasing the datapoints of a dataset from 30 to 100.\n",
    "                       \n",
    "        Down-sampling:- This technique involves decreasing the number of data of the datasets which has more number\n",
    "                        of datapoints by deleting the datapoints. for example the decreasing the datapoints of a \n",
    "                        dataset from 100 to 30\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "001e57c7-cb87-486f-b6b3-6916f275f114",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''Ans5: Data augmentation is the process of artificially increasing the datapoints by using the existing the datapoints.\n",
    "         It is typically done by doing the transformation technique.\n",
    "         \n",
    "         SMOTE is the synthetic minority oversampling technique which is used to create the datapoints between two points\n",
    "         by interpolation it creates the datapoints by using the nearest points.\n",
    "         \n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c48fff6-d2a7-4060-af3b-9819f832bd22",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''Ans6: The outliers are the values in a datasets which are much more larger, smaller then the most of the values \n",
    "         in a dataset and these outliers are very large or small then the mean value of the rest of the values of \n",
    "         the datasets.\n",
    "         for example [2,3,5,6,7,8,1425]\n",
    "         In the above example the 1425 is the outlier because 1425>>8\n",
    "         \n",
    "         The outlier affects on the mean central tendency very much the mean will be diverted towards the mean very\n",
    "         much and the variance is also affected by the outliers. Outliers can have a significant impact on the \n",
    "         performance of machine learning models. They may cause models to overfit to the training data, or lead to \n",
    "         inaccurate predictions.\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2242bd7e-b18a-4e2c-8263-debf2e77fc11",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''Ans7: The following techniques is used for filling the missing values:\n",
    "            \n",
    "            i. Mean imputation: This type of imputation is used where there is no outliers in the datapoints and\n",
    "                                will replace the missing values with the mean of the all given datapoints.\n",
    "            ii. Median impuatation: This type of impuation is used where the outliers are present in the datapoints\n",
    "                                    and the missing datapoints are replaced with the median of the given datapoints.\n",
    "            iii. Mode impuatation: This type of impuatation is used where the datapoints are categorical and the \n",
    "                                    missing values are replaced with the most occuring datapoints.\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "92fef43c-0c0d-40c3-ade8-38d216bc1460",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Ans8: The some of techniques used to determine if the missing values are random or not are:\\n            \\n            i. Visualization: This technique sketch the graph between the datasets and give the visualization of the \\n                              if the  datapoints are randomly missing or not.\\n            ii. correlation: Correlation analysis can be used to determine whether there is a relationship between \\n                             the missingness of certain variables and other variables in the dataset.\\n            iii. Imputation methods: Imputation methods can also provide insight into whether the missingness is random or not.\\n'"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''Ans8: The some of techniques used to determine if the missing values are random or not are:\n",
    "            \n",
    "            i. Visualization: This technique sketch the graph between the datasets and give the visualization of the \n",
    "                              if the  datapoints are randomly missing or not.\n",
    "            ii. correlation: Correlation analysis can be used to determine whether there is a relationship between \n",
    "                             the missingness of certain variables and other variables in the dataset.\n",
    "            iii. Imputation methods: Imputation methods can also provide insight into whether the missingness is random or not.\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88d721c1-e224-4ab3-bf7c-2c25f1b7ddb4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f01da292-2f98-4271-8ba4-c256a6418f23",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''Ans9: Imbalanced datasets can pose a challenge for machine learning models because the model may be \n",
    "         biased towards predicting the majority class and perform poorly on the minority class. \n",
    "         To evaluate the performance of a machine learning model on an imbalanced dataset, you can use \n",
    "         the following strategies:\n",
    "\n",
    "            i. Confusion matrix: The confusion matrix provides a comprehensive view of the model's performance. \n",
    "                                It shows the number of true positives, true negatives, false positives, and false \n",
    "                                negatives. You can use the confusion matrix to calculate various performance metrics \n",
    "                                such as accuracy, precision, recall, and F1-score.\n",
    "\n",
    "            ii. Precision-Recall Curve: Precision-Recall curve is a graphical representation of the trade-off \n",
    "                                                between precision and recall for different threshold values. \n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bdbc536c-e41e-40e2-b563-6d5261131d8b",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''Ans10: When dealing with an imbalanced dataset where the majority class is much larger than the minority  \n",
    "         class, it can be challenging to train a model that performs well on both classes. To address this \n",
    "         issue, one common approach is to balance the dataset by downsampling the majority class.\n",
    "\n",
    "         Random undersampling: In random undersampling, you randomly remove examples from the majority \n",
    "         class until the class distribution is balanced. One drawback of this approach is that important \n",
    "         information from the majority class may be lost.\n",
    "         \n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c04239bd-a88a-4582-a165-29db4421191e",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''Ans11: When working with a dataset that has a low percentage of occurrences or a rare event, it can be \n",
    "          challenging to train a model that can accurately predict the minority class. To address this issue, \n",
    "          one common approach is to balance the dataset by upsampling the minority class.\n",
    "\n",
    "            i. Random oversampling: In random oversampling, you randomly duplicate examples from the minority \n",
    "                                class until the class distribution is balanced. One drawback of this approach is \n",
    "                                that it can lead to overfitting and poor generalization.\n",
    "                                \n",
    "            ii. SMOTE (Synthetic Minority Over-sampling Technique): SMOTE is an oversampling technique that \n",
    "                                generates synthetic examples from the minority class. SMOTE works by selecting \n",
    "                                examples from the minority class, identifying their k-nearest neighbors, and \n",
    "                                generating synthetic examples along the line segment joining the example and its \n",
    "                                k-nearest neighbors.\n",
    "'''"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
